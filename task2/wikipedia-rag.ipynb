{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":5632975,"sourceType":"datasetVersion","datasetId":3238926},{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629},{"sourceId":6359012,"sourceType":"datasetVersion","datasetId":3662908},{"sourceId":6388591,"sourceType":"datasetVersion","datasetId":3682312},{"sourceId":6499124,"sourceType":"datasetVersion","datasetId":3756597},{"sourceId":6536614,"sourceType":"datasetVersion","datasetId":3778974},{"sourceId":6537899,"sourceType":"datasetVersion","datasetId":3779689},{"sourceId":7163608,"sourceType":"datasetVersion","datasetId":4137938},{"sourceId":7163732,"sourceType":"datasetVersion","datasetId":4138012},{"sourceId":7163761,"sourceType":"datasetVersion","datasetId":4138031},{"sourceId":7177556,"sourceType":"datasetVersion","datasetId":4148053},{"sourceId":7178507,"sourceType":"datasetVersion","datasetId":4148746}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Platypus2-70B + Wikipedia RAG\n","metadata":{}},{"cell_type":"code","source":"# Installing offline dependencies\n!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport logging\nfrom time import time\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor\nimport ctypes\nfrom functools import partial\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# For RAG\nimport faiss\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_from_disk, Dataset\n\nNUM_TITLES = 5\nMAX_SEQ_LEN = 512\nMODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n\n# For LLM\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\nfrom accelerate import init_empty_weights\nfrom accelerate.utils.modeling import set_module_tensor_to_device\nfrom safetensors.torch import load_file\n\nN_BATCHES = 5 \nMAX_CONTEXT = 2750\nMAX_LENGTH = 4096\n# Function to clean RAM & vRAM\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\n\n\nclass SentenceTransformer:\n    def __init__(self, checkpoint, device=\"cuda:0\"):\n        self.device = device\n        self.checkpoint = checkpoint\n        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    def transform(self, batch):\n        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n        return tokens.to(self.device)  \n\n    def get_dataloader(self, sentences, batch_size=32):\n        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n        dataset = Dataset.from_dict({\"text\": sentences})\n        dataset.set_transform(self.transform)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n        return dataloader\n\n    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n\n        embeddings = []\n        for batch in pbar:\n            with torch.no_grad():\n                e = self.model(**batch).pooler_output\n                e = F.normalize(e, p=2, dim=1)\n                embeddings.append(e.detach().cpu().numpy())\n        embeddings = np.concatenate(embeddings, axis=0)\n        return embeddings\n\nimport json\n\n# Specify the path to your JSON file\njson_file_path = '/kaggle/input/adobe-eval/eval.json'\n\n# Read JSON file\nwith open(json_file_path, 'r') as file:\n    data = json.load(file)\n\n# Now 'data' contains the contents of the JSON file\n# print(data)\n\n\n\ndf = pd.DataFrame(data)\n\n    start = time()\n    print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n    model = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n\n    # Get embeddings of prompts\n    f = lambda row : \" \".join([row[\"input\"]])\n    inputs = df.apply(f, axis=1).values # better results than prompt only\n    prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n\n    # Search closest sentences in the wikipedia index \n    print(f\"Loading faiss index, t={time() - start :.1f}s\")\n    faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n    faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n\n    print(f\"Starting text search, t={time() - start :.1f}s\")\n    search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n\n    print(f\"Starting context extraction, t={time() - start :.1f}s\")\n    dataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n    for i in range(len(df)):\n        df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n\n    # Free memory\n    faiss_index.reset()\n    del faiss_index, prompt_embeddings, model, dataset\n    clean_memory()\n    print(f\"Context added, t={time() - start :.1f}s\")\n    df.to_csv('fintune_with_wiki_data.csv')\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport ctypes\nfrom time import time\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# For RAG\nimport faiss\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_from_disk, Dataset\n\n# For LLM\nfrom transformers import AutoModel, AutoTokenizer\nfrom accelerate import init_empty_weights\nfrom accelerate.utils.modeling import set_module_tensor_to_device\nfrom safetensors.torch import load_file\n\nNUM_TITLES = 5\nMAX_SEQ_LEN = 512\nMODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n\nN_BATCHES = 5\nMAX_CONTEXT = 2750\nMAX_LENGTH = 4096\n\n# Function to clean RAM & vRAM\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\n\nclass SentenceTransformer:\n    def __init__(self, checkpoint, device=\"cuda:0\"):\n        self.device = device\n        self.checkpoint = checkpoint\n        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    def transform(self, batch):\n        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n        return tokens.to(self.device)\n\n    def get_dataloader(self, sentences, batch_size=32):\n        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n        dataset = Dataset.from_dict({\"text\": sentences})\n        dataset.set_transform(self.transform)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n        return dataloader\n\n    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n\n        embeddings = []\n        for batch in pbar:\n            with torch.no_grad():\n                e = self.model(**batch).pooler_output\n                e = F.normalize(e, p=2, dim=1)\n                embeddings.append(e.detach().cpu().numpy())\n        embeddings = np.concatenate(embeddings, axis=0)\n        return embeddings\n\n# Read JSON file\njson_file_path = 'eval.json'\nwith open(json_file_path, 'r') as file:\n    data = json.load(file)\n\n# Now 'data' contains the contents of the JSON file\n# print(data)\n\ndf = pd.DataFrame(data)\n\nstart = time()\nprint(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n\n# Initialize SentenceTransformer\nmodel = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n\n# Get embeddings of prompts\nf = lambda row: \" \".join([row[\"input\"]])\ninputs = df.apply(f, axis=1).values  # better results than prompt only\nprompt_embeddings = model.encode(inputs, show_progress_bar=False)\n\n# Search closest sentences in the wikipedia index\nprint(f\"Loading faiss index, t={time() - start :.1f}s\")\nfaiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\nfaiss_index = faiss.index_cpu_to_all_gpus(faiss_index)  # causes OOM, and not that long on CPU\n\nprint(f\"Starting text search, t={time() - start :.1f}s\")\nsearch_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n\nprint(f\"Starting context extraction, t={time() - start :.1f}s\")\ndataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\nfor i in range(len(df)):\n    df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n\n# Free memory\nfaiss_index.reset()\ndel faiss_index, prompt_embeddings, model, dataset\nclean_memory()\nprint(f\"Context added, t={time() - start :.1f}s\")\n\n# Save DataFrame to CSV\ndf.to_csv('fintune_with_wiki_data.csv')\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    TrainingArguments,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n)\nimport bitsandbytes as bnb\nimport json\nfrom datasets import Dataset\nimport pandas as pd\n\n# ann = json.load(open(\"/kaggle/input/finetuning/finetuning.json\"))\nmodel = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\ntokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval = []\nfor idx, r in df_eval.iterrows():\n    d = {}\n    d['input'] = r['input'][0:120]+' wikipedia data '+ r['context'][0:128]\n    \n    d['instruction'] = \"You are a content generator, you have to generate the tweet text based on the expected likes and other additional information, you also have data extracted from wikipedia.\"\n    d['output'] = pipe('<INST>'+d['input']+d['instruction']+'</INST>')['generated_text'].split('</INST>')[-1]\n    eval.append(d)","metadata":{},"execution_count":null,"outputs":[]}]}