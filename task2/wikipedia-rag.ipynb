{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Platypus2-70B + Wikipedia RAG\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# Installing offline dependencies\n","!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gc\n","import logging\n","from time import time\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor\n","import ctypes\n","from functools import partial\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","# For RAG\n","import faiss\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from datasets import load_from_disk, Dataset\n","\n","NUM_TITLES = 5\n","MAX_SEQ_LEN = 512\n","MODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n","\n","# For LLM\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\n","from accelerate import init_empty_weights\n","from accelerate.utils.modeling import set_module_tensor_to_device\n","from safetensors.torch import load_file\n","\n","N_BATCHES = 5 \n","MAX_CONTEXT = 2750\n","MAX_LENGTH = 4096\n","# Function to clean RAM & vRAM\n","def clean_memory():\n","    gc.collect()\n","    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n","    torch.cuda.empty_cache()\n","\n","\n","class SentenceTransformer:\n","    def __init__(self, checkpoint, device=\"cuda:0\"):\n","        self.device = device\n","        self.checkpoint = checkpoint\n","        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n","        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","    def transform(self, batch):\n","        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n","        return tokens.to(self.device)  \n","\n","    def get_dataloader(self, sentences, batch_size=32):\n","        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n","        dataset = Dataset.from_dict({\"text\": sentences})\n","        dataset.set_transform(self.transform)\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","        return dataloader\n","\n","    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n","        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n","        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n","\n","        embeddings = []\n","        for batch in pbar:\n","            with torch.no_grad():\n","                e = self.model(**batch).pooler_output\n","                e = F.normalize(e, p=2, dim=1)\n","                embeddings.append(e.detach().cpu().numpy())\n","        embeddings = np.concatenate(embeddings, axis=0)\n","        return embeddings\n","\n","import json\n","\n","# Specify the path to your JSON file\n","json_file_path = '/kaggle/input/adobe-eval/eval.json'\n","\n","# Read JSON file\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)\n","\n","# Now 'data' contains the contents of the JSON file\n","# print(data)\n","\n","\n","\n","df = pd.DataFrame(data)\n","\n","    start = time()\n","    print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n","    model = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n","\n","    # Get embeddings of prompts\n","    f = lambda row : \" \".join([row[\"input\"]])\n","    inputs = df.apply(f, axis=1).values # better results than prompt only\n","    prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n","\n","    # Search closest sentences in the wikipedia index \n","    print(f\"Loading faiss index, t={time() - start :.1f}s\")\n","    faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n","    faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n","\n","    print(f\"Starting text search, t={time() - start :.1f}s\")\n","    search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n","\n","    print(f\"Starting context extraction, t={time() - start :.1f}s\")\n","    dataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n","    for i in range(len(df)):\n","        df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n","\n","    # Free memory\n","    faiss_index.reset()\n","    del faiss_index, prompt_embeddings, model, dataset\n","    clean_memory()\n","    print(f\"Context added, t={time() - start :.1f}s\")\n","    df.to_csv('fintune_with_wiki_data.csv')\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc\n","import ctypes\n","from time import time\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor\n","from functools import partial\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","# For RAG\n","import faiss\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from datasets import load_from_disk, Dataset\n","\n","# For LLM\n","from transformers import AutoModel, AutoTokenizer\n","from accelerate import init_empty_weights\n","from accelerate.utils.modeling import set_module_tensor_to_device\n","from safetensors.torch import load_file\n","\n","NUM_TITLES = 5\n","MAX_SEQ_LEN = 512\n","MODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n","\n","N_BATCHES = 5\n","MAX_CONTEXT = 2750\n","MAX_LENGTH = 4096\n","\n","# Function to clean RAM & vRAM\n","def clean_memory():\n","    gc.collect()\n","    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n","    torch.cuda.empty_cache()\n","\n","class SentenceTransformer:\n","    def __init__(self, checkpoint, device=\"cuda:0\"):\n","        self.device = device\n","        self.checkpoint = checkpoint\n","        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n","        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","    def transform(self, batch):\n","        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n","        return tokens.to(self.device)\n","\n","    def get_dataloader(self, sentences, batch_size=32):\n","        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n","        dataset = Dataset.from_dict({\"text\": sentences})\n","        dataset.set_transform(self.transform)\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","        return dataloader\n","\n","    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n","        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n","        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n","\n","        embeddings = []\n","        for batch in pbar:\n","            with torch.no_grad():\n","                e = self.model(**batch).pooler_output\n","                e = F.normalize(e, p=2, dim=1)\n","                embeddings.append(e.detach().cpu().numpy())\n","        embeddings = np.concatenate(embeddings, axis=0)\n","        return embeddings\n","\n","# Read JSON file\n","json_file_path = 'eval.json'\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)\n","\n","# Now 'data' contains the contents of the JSON file\n","# print(data)\n","\n","df = pd.DataFrame(data)\n","\n","start = time()\n","print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n","\n","# Initialize SentenceTransformer\n","model = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n","\n","# Get embeddings of prompts\n","f = lambda row: \" \".join([row[\"input\"]])\n","inputs = df.apply(f, axis=1).values  # better results than prompt only\n","prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n","\n","# Search closest sentences in the wikipedia index\n","print(f\"Loading faiss index, t={time() - start :.1f}s\")\n","faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n","faiss_index = faiss.index_cpu_to_all_gpus(faiss_index)  # causes OOM, and not that long on CPU\n","\n","print(f\"Starting text search, t={time() - start :.1f}s\")\n","search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n","\n","print(f\"Starting context extraction, t={time() - start :.1f}s\")\n","dataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n","for i in range(len(df)):\n","    df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n","\n","# Free memory\n","faiss_index.reset()\n","del faiss_index, prompt_embeddings, model, dataset\n","clean_memory()\n","print(f\"Context added, t={time() - start :.1f}s\")\n","\n","# Save DataFrame to CSV\n","df.to_csv('fintune_with_wiki_data.csv')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import (\n","    TrainingArguments,\n","    LlamaForCausalLM,\n","    LlamaTokenizer,\n",")\n","import bitsandbytes as bnb\n","import json\n","from datasets import Dataset\n","import pandas as pd\n","\n","# ann = json.load(open(\"/kaggle/input/finetuning/finetuning.json\"))\n","model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n","tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eval = []\n","for idx, r in df_eval.iterrows():\n","    d = {}\n","    d['input'] = r['input'][0:120]+' wikipedia data '+ r['context'][0:128]\n","    \n","    d['instruction'] = \"You are a content generator, you have to generate the tweet text based on the expected likes and other additional information, you also have data extracted from wikipedia.\"\n","    d['output'] = pipe('<INST>'+d['input']+d['instruction']+'</INST>')['generated_text'].split('</INST>')[-1]\n","    eval.append(d)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":6169864,"sourceId":54662,"sourceType":"competition"},{"datasetId":3238926,"sourceId":5632975,"sourceType":"datasetVersion"},{"datasetId":3521629,"sourceId":6146260,"sourceType":"datasetVersion"},{"datasetId":3662908,"sourceId":6359012,"sourceType":"datasetVersion"},{"datasetId":3682312,"sourceId":6388591,"sourceType":"datasetVersion"},{"datasetId":3756597,"sourceId":6499124,"sourceType":"datasetVersion"},{"datasetId":3778974,"sourceId":6536614,"sourceType":"datasetVersion"},{"datasetId":3779689,"sourceId":6537899,"sourceType":"datasetVersion"},{"datasetId":4137938,"sourceId":7163608,"sourceType":"datasetVersion"},{"datasetId":4138012,"sourceId":7163732,"sourceType":"datasetVersion"},{"datasetId":4138031,"sourceId":7163761,"sourceType":"datasetVersion"},{"datasetId":4148053,"sourceId":7177556,"sourceType":"datasetVersion"},{"datasetId":4148746,"sourceId":7178507,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7 (main, May 29 2023, 13:51:48) [GCC 12.2.0]"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
