## Task 2

### Training Scripts and Inference

#### 1. Initial Captioning: Blip Inference 
Initial inference of images, videos and gif files for generating captions and prompts to give to LLM based models for finetuning and generating quality captions. Use this to generate data for `finetuning.json` file to be stored in the data folder.

```
>> cd ./task2/blip-inference/
>> git clone https://github.com/salesforce/BLIP
>> cd ../..
>> python3 ./task2/blip-inference/blip-inference-data.py --data-path=./data/complete_username_data.xlsx --username-data=./data/username_data.csv
```

Note: 
Install these libraries like this to use bits and bytes and other parts used in file. Also for this task try to use a system with a single GPU memory of atleast 10 GB and RAM of about 16 GB. We used Kaggle free GB notebooks for running these tasks on both test and train dataset.
```
>> pip install -i https://test.pypi.org/simple/ bitsandbytes
>> pip install flash-attn --no-build-isolation
```

#### 2. Post Processing
Here we will try to use LLM based approaches to post process the captions generated by the BLIP model (1 and 2) to generate prompts and finetuning data that can be directly used to finetune and infer the new LLM models like LLAMA, FastGPT and others to generate better and proper captions for the Twitter posts.


##### (A) LLAMA Adapter (The Best Captions were generated with this approach)

##### (B) Incontext Learning
We leveraged Incontext Learning to enhance the generation of tweet text. In conjunction with the input prompt, we equipped the Llama-2 models with three illustrative examples sourced from the training set. We employed SBERT embeddings of the input metadata, along with FAISS for similarity search, to retrieve the three most relevant tweet metadata entries similar to the one we are evaluating.

To utilize this inference tool, use the following command:
```
>> python ./task2/incontext_learning/incontext-learning-inference.py --input path/to/input.json --eval path/to/eval.json
```
This command will initiate the inference process and evaluate the specified input and evaluation JSON files.

##### (C) Lora Fintuning
We used Low Rank Adaptation (Lora) a parameter efficient fine tuning approach using the peft library from huggingface. We finetuned Llama-2 7B model using this approach on the dataset. We converted the dataset in the alpaca format to instruction fine-tune the model. We used the following command to fine-tune the model.

```
>> python ./task2/lora/lora_finetuning.py  
```
To get inference on the finetune, use the following command
```
>> python ./task2/lora/lora_inference.py  
```
##### (D) Fast-GPT
We used Fast-GPT training and eval scripts to get quick inference results from the training from pytorch's [Fast-GPT](https://github.com/pytorch-labs/gpt-fast#BSD-3-Clause-1-ov-file) licensed by Meta under BSD-3 clase Licence. 
The repo `task2/fast_GPT` is a clone of the PyTorch repo and we wrote an eval script for understanding how to use the script for the finetuning and later inference of the model. If more work is done on this area, we may significantly reduce the training and the inference 

To use this for inference and training, first:

```
>> cd ./task2/fast_GPT
>> git clone https://github.com/pytorch-labs/gpt-fast
>> mv GPT-Fast gpt_fast
>> cd ../../
>> python eval.py # Try to making the path changes first for the training and eval scripts 
```

Note: While we experimented with this approach, the we did not use this for the inference of the final solution as it can be a bit unreliable as it used `torch.compile` and other experimental features. 
